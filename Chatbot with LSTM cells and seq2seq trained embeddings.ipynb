{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJAWnBFlkE2w"
   },
   "source": [
    "# LSTM Bot\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, you will build a chatbot that can converse with you at the command line. The chatbot will use a Sequence to Sequence text generation architecture with an LSTM as it's memory unit. You will also learn to use pretrained word embeddings to improve the performance of the model. At the conclusion of the project, you will be able to show your chatbot to potential employers.\n",
    "\n",
    "Additionally, you have the option to use pretrained word embeddings in your model. We have loaded Brown Embeddings from Gensim in the starter code below. You can compare the performance of your model with pre-trained embeddings against a model without the embeddings.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "A sequence to sequence model (Seq2Seq) has two components:\n",
    "- An Encoder consisting of an embedding layer and LSTM unit.\n",
    "- A Decoder consisting of an embedding layer, LSTM unit, and linear output unit.\n",
    "\n",
    "The Seq2Seq model works by accepting an input into the Encoder, passing the hidden state from the Encoder to the Decoder, which the Decoder uses to output a series of token predictions.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- Pytorch\n",
    "- Numpy\n",
    "- Pandas\n",
    "- NLTK\n",
    "- Gzip\n",
    "- Gensim\n",
    "\n",
    "\n",
    "Please choose a dataset from the Torchtext website. We recommend looking at the Squad dataset first. Here is a link to the website where you can view your options:\n",
    "\n",
    "- https://pytorch.org/text/stable/datasets.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eg81uNTWixbi",
    "outputId": "9c0f9eda-75fb-4526-e9b6-f9a76eeeb007"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding word 0 to our vocabulary.\n",
      "Adding word 15000 to our vocabulary.\n",
      "Adding word 30000 to our vocabulary.\n",
      "Adding word 45000 to our vocabulary.\n",
      "Adding word 60000 to our vocabulary.\n",
      "Adding word 75000 to our vocabulary.\n",
      "Adding word 90000 to our vocabulary.\n",
      "Adding word 105000 to our vocabulary.\n",
      "Adding word 120000 to our vocabulary.\n",
      "Adding word 135000 to our vocabulary.\n",
      "Adding word 150000 to our vocabulary.\n",
      "Adding word 165000 to our vocabulary.\n",
      "Adding word 180000 to our vocabulary.\n",
      "Adding word 195000 to our vocabulary.\n",
      "Adding word 210000 to our vocabulary.\n",
      "Adding word 225000 to our vocabulary.\n",
      "Adding word 240000 to our vocabulary.\n",
      "Adding word 255000 to our vocabulary.\n",
      "Adding word 270000 to our vocabulary.\n",
      "Adding word 285000 to our vocabulary.\n",
      "Adding word 300000 to our vocabulary.\n",
      "Adding word 315000 to our vocabulary.\n",
      "Adding word 330000 to our vocabulary.\n",
      "Adding word 345000 to our vocabulary.\n",
      "Adding word 360000 to our vocabulary.\n",
      "Adding word 375000 to our vocabulary.\n",
      "Adding word 390000 to our vocabulary.\n",
      "Adding word 405000 to our vocabulary.\n",
      "Adding word 420000 to our vocabulary.\n",
      "Adding word 435000 to our vocabulary.\n",
      "Adding word 450000 to our vocabulary.\n",
      "Adding word 465000 to our vocabulary.\n",
      "Adding word 480000 to our vocabulary.\n",
      "Adding word 495000 to our vocabulary.\n",
      "Adding word 510000 to our vocabulary.\n",
      "Adding word 525000 to our vocabulary.\n",
      "Adding word 540000 to our vocabulary.\n",
      "Adding word 555000 to our vocabulary.\n",
      "Adding word 570000 to our vocabulary.\n",
      "Adding word 585000 to our vocabulary.\n",
      "Adding word 600000 to our vocabulary.\n",
      "Adding word 615000 to our vocabulary.\n",
      "Adding word 630000 to our vocabulary.\n",
      "Adding word 645000 to our vocabulary.\n",
      "Adding word 660000 to our vocabulary.\n",
      "Adding word 675000 to our vocabulary.\n",
      "Adding word 690000 to our vocabulary.\n",
      "Adding word 705000 to our vocabulary.\n",
      "Adding word 720000 to our vocabulary.\n",
      "Adding word 735000 to our vocabulary.\n",
      "Adding word 750000 to our vocabulary.\n",
      "Adding word 765000 to our vocabulary.\n",
      "Adding word 780000 to our vocabulary.\n",
      "Adding word 795000 to our vocabulary.\n",
      "Adding word 810000 to our vocabulary.\n",
      "Adding word 825000 to our vocabulary.\n",
      "Adding word 840000 to our vocabulary.\n",
      "Adding word 855000 to our vocabulary.\n",
      "Adding word 870000 to our vocabulary.\n",
      "Adding word 885000 to our vocabulary.\n",
      "Adding word 900000 to our vocabulary.\n",
      "Adding word 915000 to our vocabulary.\n",
      "Adding word 930000 to our vocabulary.\n",
      "Adding word 945000 to our vocabulary.\n",
      "Adding word 960000 to our vocabulary.\n",
      "Adding word 975000 to our vocabulary.\n",
      "Adding word 990000 to our vocabulary.\n",
      "Adding word 1005000 to our vocabulary.\n",
      "Adding word 1020000 to our vocabulary.\n",
      "Adding word 1035000 to our vocabulary.\n",
      "Adding word 1050000 to our vocabulary.\n",
      "Adding word 1065000 to our vocabulary.\n",
      "Adding word 1080000 to our vocabulary.\n",
      "Adding word 1095000 to our vocabulary.\n",
      "Adding word 1110000 to our vocabulary.\n",
      "Adding word 1125000 to our vocabulary.\n",
      "Adding word 1140000 to our vocabulary.\n",
      "Adding word 1155000 to our vocabulary.\n",
      "Adding word 1170000 to our vocabulary.\n",
      "Word count in vocab is 30363. Removed 22159 words during cleanup.\n",
      "Data frame contains 86821 rows.\n",
      "Data frame after row cleanup contains 26620 rows.\n",
      "create train, test and validation data sets ...\n",
      "Train set of length: 18634\n",
      "Test set of length: 5990\n",
      "Valid set of length: 1996\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "from dataset_helper import load_df, prepare_text, train_test_split\n",
    "\n",
    "train, dev = load_df()\n",
    "v, token_df = prepare_text(count_limit=2, min_length=2, max_length=13, stage='train') # dev or train\n",
    "print(f'create train, test and validation data sets ...')\n",
    "train_set, test_set, valid_set = train_test_split(token_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: <SOS> on what date was the ussr dissolved <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "A: <SOS> december 26 1991 <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "\n",
      "Q: <SOS> why did queen victoria want to take over other countries <EOS> <PAD>\n",
      "A: <SOS> protecting native peoples from more aggressive powers or cruel rulers <EOS> <PAD> \n",
      "\n",
      "Q: <SOS> what do young birds form attachments to <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "A: <SOS> potential breeding sites <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "\n",
      "Q: <SOS> who is toni morrison <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "A: <SOS> nobel prize winning novelist <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "\n",
      "Q: <SOS> who wrote the book robopocalypse is based on <EOS> <PAD> <PAD> <PAD>\n",
      "A: <SOS> daniel h wilson <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "\n",
      "Q: <SOS> what team did the cincinnati red stockings become <EOS> <PAD> <PAD> <PAD>\n",
      "A: <SOS> the atlanta braves <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "\n",
      "Q: <SOS> what type of rudder was introduced during this period <EOS> <PAD> <PAD>\n",
      "A: <SOS> stern post <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "\n",
      "Q: <SOS> what is the largest rating of an electric motor <EOS> <PAD> <PAD>\n",
      "A: <SOS> 100 megawatts <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "\n",
      "Q: <SOS> what countries did kievan rus become <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "A: <SOS> modern russia ukraine and belarus <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "\n",
      "Q: <SOS> where was henry iii crowned <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "A: <SOS> gloucester cathedral <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print first 10 QnAs\n",
    "for i in range(10):\n",
    "    ex_q = train_set.iloc[i, 0]\n",
    "    ex_a = train_set.iloc[i, 1]\n",
    "    ex_question = [w for w in v.index2word(ex_q) if w!= '<UNK>']\n",
    "    ex_answer = [w for w in v.index2word(ex_a) if w!= '<UNK>']\n",
    "    # Finally, write out an answer for user\n",
    "    print(\"Q:\", \" \".join(ex_question))\n",
    "    print(\"A:\", \" \".join(ex_answer), \"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# define parameters\n",
    "input_size = len(v.words)\n",
    "output_size = len(v.words)\n",
    "embedding_size = 256\n",
    "hidden_size = 512\n",
    "lstm_layer = 2\n",
    "dropout = 0.3\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "clip = 1\n",
    "BATCH_SIZE = 128\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created `train_dataloader` with 145 batches!\n",
      "Created `test_dataloader` with 46 batches!\n",
      "Created `test_dataloader` with 15 batches!\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(30363, 256)\n",
      "    (lstm): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.3)\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(30363, 256)\n",
      "    (lstm): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.3)\n",
      "    (lin_out): Linear(in_features=512, out_features=30363, bias=True)\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from dataset_helper import get_dataloader\n",
    "from encoder import Encoder\n",
    "from decoder import Decoder\n",
    "from seq2seq import Seq2Seq\n",
    "import torch\n",
    "\n",
    "train_dataloader, test_dataloader, valid_dataloader = get_dataloader(train_set, test_set, valid_set, BATCH_SIZE)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# instantiate encoder and decoder classes\n",
    "enc = Encoder(input_size, hidden_size, embedding_size, lstm_layer, dropout, BATCH_SIZE).to(device)\n",
    "dec = Decoder(input_size, hidden_size, output_size, embedding_size, lstm_layer, dropout).to(device)\n",
    "# instantiate seq2seq model\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 0 | \tTrain Loss: 8.197 | \t Val. Loss: 7.696\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 11\u001B[0m\n\u001B[1;32m      9\u001B[0m criterion \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss(ignore_index\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m)\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# train loop\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m \u001B[43mtrain_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclip\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_out_path\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Udacity Training/seq2seq/helpers.py:21\u001B[0m, in \u001B[0;36mtrain_loop\u001B[0;34m(model, train_dl, test_dl, optimizer, criterion, clip, device, epochs, model_out_path)\u001B[0m\n\u001B[1;32m     17\u001B[0m best_valid_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minf\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;66;03m# model, iterator, optimizer, criterion, clip\u001B[39;00m\n\u001B[0;32m---> 21\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclip\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m     valid_loss \u001B[38;5;241m=\u001B[39m evaluate(model, test_dl, criterion, device)\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m valid_loss \u001B[38;5;241m<\u001B[39m best_valid_loss:\n",
      "File \u001B[0;32m~/Udacity Training/seq2seq/helpers.py:50\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, iterator, optimizer, criterion, clip, device)\u001B[0m\n\u001B[1;32m     48\u001B[0m trg_flatten \u001B[38;5;241m=\u001B[39m trg[\u001B[38;5;241m1\u001B[39m:]\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     49\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs_flatten, trg_flatten)\n\u001B[0;32m---> 50\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;66;03m# gradient clipping\u001B[39;00m\n\u001B[1;32m     52\u001B[0m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mclip_grad_norm_(model\u001B[38;5;241m.\u001B[39mparameters(), clip)\n",
      "File \u001B[0;32m~/.pyenv/versions/seq2seq/lib/python3.11/site-packages/torch/_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    486\u001B[0m     )\n\u001B[0;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/seq2seq/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    195\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    197\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    199\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 200\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    201\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    202\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from helpers import evaluate, train_loop\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "model_out_path = 'qna-model.pt'\n",
    "# definer optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# ignore padding index when calculating the loss (<PAD>=3 in vocab)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=3)\n",
    "# train loop\n",
    "train_loop(model, train_dataloader, test_dataloader, optimizer, criterion, clip, device, epochs, model_out_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 7.703\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on data it has never seen\n",
    "model_out_path = 'qna-model.pt'\n",
    "model.load_state_dict(torch.load(model_out_path))\n",
    "valid_loss = evaluate(model, valid_dataloader, criterion, device)\n",
    "print(f'Validation Loss: {valid_loss:.3f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 'exit' to finish the chat.\n",
      " ------------------------------ \n",
      "\n",
      "['<SOS>', 'the', '<EOS>', '<EOS>', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "# inference, load model\n",
    "model = Seq2Seq(enc, dec, device)\n",
    "model.load_state_dict(torch.load(model_out_path))\n",
    "model.eval\n",
    "\n",
    "# define the input question\n",
    "# question = \"What does the urban education institute help run?\"\n",
    "print(\"Type 'exit' to finish the chat.\\n\", \"-\"*30, '\\n')\n",
    "while (True):\n",
    "    question = input(\"> \")\n",
    "    if question.strip() == \"exit\":\n",
    "        break\n",
    "    # clean and tokenize the input question\n",
    "    src = v.word2index(v.clean_text(question))\n",
    "    # convert the tokenized question to a tensor and add a batch dimension\n",
    "    src = torch.tensor(src, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    # generate the answer using the model\n",
    "    output = model(src, None, 0)\n",
    "    # convert the output tensor to a list of token IDs\n",
    "    preds = output.argmax(dim=2).tolist()[0]\n",
    "    # convert the token IDs to tokens\n",
    "    answer = v.index2word(preds)\n",
    "    # print the predicted answer\n",
    "    print(answer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(Starter Code) LSTM Bot",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}