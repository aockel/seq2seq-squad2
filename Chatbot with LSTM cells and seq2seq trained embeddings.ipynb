{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJAWnBFlkE2w"
   },
   "source": [
    "# LSTM Bot\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, you will build a chatbot that can converse with you at the command line. The chatbot will use a Sequence to Sequence text generation architecture with an LSTM as it's memory unit. You will also learn to use pretrained word embeddings to improve the performance of the model. At the conclusion of the project, you will be able to show your chatbot to potential employers.\n",
    "\n",
    "Additionally, you have the option to use pretrained word embeddings in your model. We have loaded Brown Embeddings from Gensim in the starter code below. You can compare the performance of your model with pre-trained embeddings against a model without the embeddings.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "A sequence to sequence model (Seq2Seq) has two components:\n",
    "- An Encoder consisting of an embedding layer and LSTM unit.\n",
    "- A Decoder consisting of an embedding layer, LSTM unit, and linear output unit.\n",
    "\n",
    "The Seq2Seq model works by accepting an input into the Encoder, passing the hidden state from the Encoder to the Decoder, which the Decoder uses to output a series of token predictions.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- Pytorch\n",
    "- Numpy\n",
    "- Pandas\n",
    "- NLTK\n",
    "- Gzip\n",
    "- Gensim\n",
    "\n",
    "\n",
    "Please choose a dataset from the Torchtext website. We recommend looking at the Squad dataset first. Here is a link to the website where you can view your options:\n",
    "\n",
    "- https://pytorch.org/text/stable/datasets.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eg81uNTWixbi",
    "outputId": "9c0f9eda-75fb-4526-e9b6-f9a76eeeb007"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding word 0 to our vocabulary.\n",
      "Adding word 15000 to our vocabulary.\n",
      "Adding word 30000 to our vocabulary.\n",
      "Adding word 45000 to our vocabulary.\n",
      "Adding word 60000 to our vocabulary.\n",
      "Adding word 75000 to our vocabulary.\n",
      "Word count in vocab is 5579. Removed 5059 words during cleanup.\n",
      "Data frame contains 5928 rows.\n",
      "Data frame after row cleanup contains 1046 rows.\n",
      "create train, test and validation data sets ...\n",
      "Train set of length: 732\n",
      "Test set of length: 236\n",
      "Valid set of length: 78\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "from dataset_helper import load_df, prepare_text, train_test_split\n",
    "\n",
    "train, dev = load_df()\n",
    "v, token_df = prepare_text(count_limit=2, min_length=2, max_length=13, stage='dev') # dev or train\n",
    "print(f'create train, test and validation data sets ...')\n",
    "train_set, test_set, valid_set = train_test_split(token_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: <SOS> petrologists identify rock samples in the field and where else <EOS> <PAD>\n",
      "A: <SOS> the laboratory <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "\n",
      "Q: <SOS> how high are victoria s alpine regions <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "A: <SOS> 2 000 m <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "\n",
      "Q: <SOS> what do some believe the treaty of versailles assisted in <EOS> <PAD>\n",
      "A: <SOS> adolf hitler s rise to power <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "\n",
      "Q: <SOS> friedrich ratzel thought imperialism was what for the country <EOS> <PAD> <PAD>\n",
      "A: <SOS> geographical societies in europe <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "\n",
      "Q: <SOS> what did this agreement do <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "A: <SOS> granted the huguenots substantial religious political and military autonomy <EOS> <PAD> <PAD> \n",
      "\n",
      "Q: <SOS> what is negatively correlated to the duration of economic growth <EOS> <PAD>\n",
      "A: <SOS> inequality in wealth and income <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "\n",
      "Q: <SOS> what describes the proportionality of acceleration to force and mass <EOS> <PAD>\n",
      "A: <SOS> newton s second law <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "\n",
      "Q: <SOS> what does the w and z boson exchange create <EOS> <PAD> <PAD>\n",
      "A: <SOS> weak force <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "\n",
      "Q: <SOS> who thought the world could be split into climatic zones <EOS> <PAD>\n",
      "A: <SOS> geographic scholars <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "\n",
      "Q: <SOS> who formed the universal theory of gravitation <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "A: <SOS> isaac newton <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print first 10 QnAs\n",
    "for i in range(10):\n",
    "    ex_q = train_set.iloc[i, 0]\n",
    "    ex_a = train_set.iloc[i, 1]\n",
    "    ex_question = [w for w in v.index2word(ex_q) if w!= '<UNK>']\n",
    "    ex_answer = [w for w in v.index2word(ex_a) if w!= '<UNK>']\n",
    "    # Finally, write out an answer for user\n",
    "    print(\"Q:\", \" \".join(ex_question))\n",
    "    print(\"A:\", \" \".join(ex_answer), \"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# define parameters\n",
    "input_size = len(v.words)\n",
    "output_size = len(v.words)\n",
    "embedding_size = 128 # 256\n",
    "hidden_size = 32 # 256\n",
    "lstm_layer = 2\n",
    "dropout = 0.5\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "clip = 1\n",
    "BATCH_SIZE = 32\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created `train_dataloader` with 22 batches!\n",
      "Created `test_dataloader` with 7 batches!\n",
      "Created `test_dataloader` with 2 batches!\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(5579, 128)\n",
      "    (lstm): LSTM(128, 32, num_layers=2, batch_first=True, dropout=0.5)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(5579, 128)\n",
      "    (lstm): LSTM(128, 32, num_layers=2, batch_first=True, dropout=0.5)\n",
      "    (lin_out): Linear(in_features=32, out_features=5579, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from dataset_helper import get_dataloader\n",
    "from encoder import Encoder\n",
    "from decoder import Decoder\n",
    "from seq2seq import Seq2Seq\n",
    "import torch\n",
    "\n",
    "train_dataloader, test_dataloader, valid_dataloader = get_dataloader(train_set, test_set, valid_set, BATCH_SIZE)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# instantiate encoder and decoder classes\n",
    "enc = Encoder(input_size, hidden_size, embedding_size, lstm_layer, dropout, BATCH_SIZE).to(device)\n",
    "dec = Decoder(input_size, hidden_size, output_size, embedding_size, lstm_layer, dropout).to(device)\n",
    "# instantiate seq2seq model\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 0 | \tTrain Loss: 8.627 | \t Val. Loss: 8.505\n",
      "\tEpoch: 5 | \tTrain Loss: 6.242 | \t Val. Loss: 6.987\n",
      "\tEpoch: 10 | \tTrain Loss: 6.099 | \t Val. Loss: 7.093\n",
      "\tEpoch: 15 | \tTrain Loss: 6.055 | \t Val. Loss: 7.210\n",
      "\tEpoch: 20 | \tTrain Loss: 6.013 | \t Val. Loss: 7.278\n",
      "\tEpoch: 25 | \tTrain Loss: 5.973 | \t Val. Loss: 7.333\n",
      "\tEpoch: 30 | \tTrain Loss: 5.947 | \t Val. Loss: 7.405\n",
      "\tEpoch: 35 | \tTrain Loss: 5.917 | \t Val. Loss: 7.391\n",
      "\tEpoch: 40 | \tTrain Loss: 5.872 | \t Val. Loss: 7.422\n",
      "\tEpoch: 45 | \tTrain Loss: 5.841 | \t Val. Loss: 7.509\n",
      "\tEpoch: 50 | \tTrain Loss: 5.799 | \t Val. Loss: 7.595\n",
      "\tEpoch: 55 | \tTrain Loss: 5.754 | \t Val. Loss: 7.535\n",
      "\tEpoch: 60 | \tTrain Loss: 5.721 | \t Val. Loss: 7.616\n",
      "\tEpoch: 65 | \tTrain Loss: 5.677 | \t Val. Loss: 7.583\n",
      "\tEpoch: 70 | \tTrain Loss: 5.645 | \t Val. Loss: 7.642\n",
      "\tEpoch: 75 | \tTrain Loss: 5.594 | \t Val. Loss: 7.670\n",
      "\tEpoch: 80 | \tTrain Loss: 5.556 | \t Val. Loss: 7.666\n",
      "\tEpoch: 85 | \tTrain Loss: 5.505 | \t Val. Loss: 7.702\n",
      "\tEpoch: 90 | \tTrain Loss: 5.476 | \t Val. Loss: 7.651\n",
      "\tEpoch: 95 | \tTrain Loss: 5.434 | \t Val. Loss: 7.720\n"
     ]
    }
   ],
   "source": [
    "from helpers import evaluate, train_loop\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "model_out_path = 'qna-model.pt'\n",
    "# definer optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# ignore padding index when calculating the loss (<PAD>=3 in vocab)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=3)\n",
    "# train loop\n",
    "train_loop(model, train_dataloader, test_dataloader, optimizer, criterion, clip, device, epochs, model_out_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 7.167\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on data it has never seen\n",
    "model_out_path = 'qna-model.pt'\n",
    "model.load_state_dict(torch.load(model_out_path))\n",
    "valid_loss = evaluate(model, valid_dataloader, criterion, device)\n",
    "print(f'Validation Loss: {valid_loss:.3f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 'exit' to finish the chat.\n",
      " ------------------------------ \n",
      "\n",
      "['<SOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "# inference, load model\n",
    "model = Seq2Seq(enc, dec, device)\n",
    "model.load_state_dict(torch.load(model_out_path))\n",
    "model.eval\n",
    "\n",
    "# define the input question\n",
    "# question = \"What does the urban education institute help run?\"\n",
    "print(\"Type 'exit' to finish the chat.\\n\", \"-\"*30, '\\n')\n",
    "while (True):\n",
    "    question = input(\"> \")\n",
    "    if question.strip() == \"exit\":\n",
    "        break\n",
    "    # clean and tokenize the input question\n",
    "    src = v.word2index(v.clean_text(question))\n",
    "    # convert the tokenized question to a tensor and add a batch dimension\n",
    "    src = torch.tensor(src, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    # generate the answer using the model\n",
    "    output = model(src=src, trg=None, teaching=0, max_len=13)\n",
    "    # convert the output tensor to a list of token IDs\n",
    "    preds = output.argmax(dim=2).tolist()[0]\n",
    "    # convert the token IDs to tokens\n",
    "    answer = v.index2word(preds)\n",
    "    # print the predicted answer\n",
    "    print(answer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(Starter Code) LSTM Bot",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}