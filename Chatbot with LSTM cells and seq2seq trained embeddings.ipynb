{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJAWnBFlkE2w"
   },
   "source": [
    "# LSTM Bot\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, you will build a chatbot that can converse with you at the command line. The chatbot will use a Sequence to Sequence text generation architecture with an LSTM as it's memory unit. You will also learn to use pretrained word embeddings to improve the performance of the model. At the conclusion of the project, you will be able to show your chatbot to potential employers.\n",
    "\n",
    "Additionally, you have the option to use pretrained word embeddings in your model. We have loaded Brown Embeddings from Gensim in the starter code below. You can compare the performance of your model with pre-trained embeddings against a model without the embeddings.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "A sequence to sequence model (Seq2Seq) has two components:\n",
    "- An Encoder consisting of an embedding layer and LSTM unit.\n",
    "- A Decoder consisting of an embedding layer, LSTM unit, and linear output unit.\n",
    "\n",
    "The Seq2Seq model works by accepting an input into the Encoder, passing the hidden state from the Encoder to the Decoder, which the Decoder uses to output a series of token predictions.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- Pytorch\n",
    "- Numpy\n",
    "- Pandas\n",
    "- NLTK\n",
    "- Gzip\n",
    "- Gensim\n",
    "\n",
    "\n",
    "Please choose a dataset from the Torchtext website. We recommend looking at the Squad dataset first. Here is a link to the website where you can view your options:\n",
    "\n",
    "- https://pytorch.org/text/stable/datasets.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eg81uNTWixbi",
    "outputId": "9c0f9eda-75fb-4526-e9b6-f9a76eeeb007"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/q439310/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data frame contains 20000 rows.\n",
      "Dev data frame contains 5928 rows.\n",
      "Adding word 0 to our vocabulary.\n",
      "Adding word 50000 to our vocabulary.\n",
      "Adding word 100000 to our vocabulary.\n",
      "Adding word 150000 to our vocabulary.\n",
      "Word count in vocab is now 9091, removed 5890 words during cleanup.\n",
      "Data frame contains 20000 rows.\n",
      "Data frame after row cleanup contains 3391 rows.\n",
      "create train, test and validation data sets ...\n",
      "Train set of length: 2374\n",
      "Test set of length: 763\n",
      "Valid set of length: 254\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "from dataset_helper import prepare_text, train_test_split\n",
    "# load and prepare data\n",
    "v, token_df = prepare_text(max_rows_train_set=20000, count_limit=2, min_length=3, max_length=12, stage='train') # dev or train\n",
    "print(f'create train, test and validation data sets ...')\n",
    "train_set, test_set, valid_set = train_test_split(token_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: <SOS> provision govern democrat feder yugoslavia assembl <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "A: <SOS> 7 march 1945 <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "\n",
      "Q: <SOS> much money use strengthen construct school <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "A: <SOS> 400 000 yuan <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "\n",
      "Q: <SOS> mani student new york citi public school <EOS> <PAD> <PAD> <PAD>\n",
      "A: <SOS> 1 1 million <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "\n",
      "Q: <SOS> mani peopl watch first episod american idol second season <EOS> <PAD>\n",
      "A: <SOS> 26 5 million <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "\n",
      "Q: <SOS> televis station region headquart plymouth <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "A: <SOS> bbc south west <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "\n",
      "Q: <SOS> latitud guinea bissau most lie <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "A: <SOS> 11 13 n <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "\n",
      "Q: <SOS> accord articl tibet remain jurisdict <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "A: <SOS> central govern china <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "\n",
      "Q: <SOS> threat caus gordon publish articl <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "A: <SOS> resumpt work railway <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "\n",
      "Q: <SOS> reader digest includ kill mockingbird program <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "A: <SOS> book month club <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "\n",
      "Q: <SOS> countri abl fli saint helena use airport <EOS> <PAD> <PAD> <PAD>\n",
      "A: <SOS> south africa uk <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "\n",
      "Q: <SOS> english equival term abbrevi j c b <EOS> <PAD> <PAD> <PAD>\n",
      "A: <SOS> bachelor canon law <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "\n",
      "Q: <SOS> beyonc√© name daughter <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "A: <SOS> blue ivi carter <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "\n",
      "Q: <SOS> first isp establish <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "A: <SOS> australia unit state <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "\n",
      "Q: <SOS> u presid kany critic event hurrican katrina <EOS> <PAD> <PAD> <PAD>\n",
      "A: <SOS> georg w bush <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "\n",
      "Q: <SOS> somali nation televis offici launch <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "A: <SOS> april 4 2011 <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print first 15 QnAs\n",
    "for i in range(15):\n",
    "    ex_q = train_set.iloc[i, 0]\n",
    "    ex_a = train_set.iloc[i, 1]\n",
    "    ex_question = [w for w in v.index2word(ex_q) if w!= '<UNK>']\n",
    "    ex_answer = [w for w in v.index2word(ex_a) if w!= '<UNK>']\n",
    "    # Finally, write out an answer for user\n",
    "    print(\"Q:\", \" \".join(ex_question))\n",
    "    print(\"A:\", \" \".join(ex_answer), \"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# define parameters\n",
    "input_size = len(v.words)\n",
    "output_size = len(v.words)\n",
    "embedding_size = 300\n",
    "hidden_size = 512\n",
    "lstm_layer = 2\n",
    "dropout = 0.3\n",
    "learning_rate = 0.02\n",
    "epochs = 200\n",
    "clip = 1  # something between 1 and 5 as a starting point\n",
    "BATCH_SIZE = 64\n",
    "teaching_ratio = 0.5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created `train_dataloader` with 37 batches!\n",
      "Created `test_dataloader` with 11 batches!\n",
      "Created `test_dataloader` with 3 batches!\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(9091, 300)\n",
      "    (lstm): LSTM(300, 512, num_layers=2, batch_first=True, dropout=0.3)\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(9091, 300)\n",
      "    (lstm): LSTM(300, 512, num_layers=2, dropout=0.3)\n",
      "    (lin_out): Linear(in_features=512, out_features=9091, bias=True)\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (softmax): LogSoftmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from dataset_helper import get_dataloader\n",
    "from encoder import Encoder\n",
    "from decoder import Decoder\n",
    "from seq2seq import Seq2Seq\n",
    "import torch\n",
    "\n",
    "train_dataloader, test_dataloader, valid_dataloader = get_dataloader(train_set, test_set, valid_set, BATCH_SIZE)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# instantiate encoder and decoder classes\n",
    "enc = Encoder(input_size, hidden_size, embedding_size, lstm_layer, dropout, BATCH_SIZE).to(device)\n",
    "dec = Decoder(input_size, hidden_size, output_size, embedding_size, lstm_layer, dropout).to(device)\n",
    "# instantiate seq2seq model\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch:   0 | \t Train Loss: 3.634 | \t  lr: 0.02\n",
      "\tEpoch:   1 | \t Train Loss: 2.833 | \t  lr: 0.02\n",
      "\tEpoch:   2 | \t Train Loss: 2.697 | \t  lr: 0.02\n",
      "\tEpoch:   3 | \t Train Loss: 2.639 | \t  lr: 0.02\n",
      "\tEpoch:   4 | \t Train Loss: 2.559 | \t  lr: 0.02\n",
      "\tEpoch:   5 | \t Train Loss: 2.504 | \t  lr: 0.02\n",
      "\tEpoch:   6 | \t Train Loss: 2.463 | \t  lr: 0.02\n",
      "\tEpoch:   7 | \t Train Loss: 2.454 | \t  lr: 0.02\n",
      "\tEpoch:   8 | \t Train Loss: 2.441 | \t  lr: 0.02\n",
      "\tEpoch:   9 | \t Train Loss: 2.453 | \t  lr: 0.02\n",
      "\tEpoch:  10 | \t Train Loss: 2.444 | \t  lr: 0.02\n",
      "\tEpoch:  11 | \t Train Loss: 2.428 | \t  lr: 0.02\n",
      "\tEpoch:  12 | \t Train Loss: 2.359 | \t  lr: 0.02\n",
      "\tEpoch:  13 | \t Train Loss: 2.334 | \t  lr: 0.02\n",
      "\tEpoch:  14 | \t Train Loss: 2.301 | \t  lr: 0.02\n",
      "\tEpoch:  15 | \t Train Loss: 2.292 | \t  lr: 0.02\n",
      "\tEpoch:  16 | \t Train Loss: 2.265 | \t  lr: 0.02\n",
      "\tEpoch:  17 | \t Train Loss: 2.237 | \t  lr: 0.02\n",
      "\tEpoch:  18 | \t Train Loss: 2.221 | \t  lr: 0.02\n",
      "\tEpoch:  19 | \t Train Loss: 2.203 | \t  lr: 0.02\n",
      "\tEpoch:  20 | \t Train Loss: 2.260 | \t  lr: 0.02\n",
      "\tEpoch:  21 | \t Train Loss: 2.201 | \t  lr: 0.02\n",
      "\tEpoch:  22 | \t Train Loss: 2.183 | \t  lr: 0.02\n",
      "\tEpoch:  23 | \t Train Loss: 2.186 | \t  lr: 0.02\n",
      "\tEpoch:  24 | \t Train Loss: 2.182 | \t  lr: 0.02\n",
      "\tEpoch:  25 | \t Train Loss: 2.185 | \t  lr: 0.02\n",
      "\tEpoch:  26 | \t Train Loss: 2.174 | \t  lr: 0.02\n",
      "\tEpoch:  27 | \t Train Loss: 2.142 | \t  lr: 0.02\n",
      "\tEpoch:  28 | \t Train Loss: 2.128 | \t  lr: 0.02\n",
      "\tEpoch:  29 | \t Train Loss: 2.120 | \t  lr: 0.02\n",
      "\tEpoch:  30 | \t Train Loss: 2.100 | \t  lr: 0.02\n",
      "\tEpoch:  31 | \t Train Loss: 2.109 | \t  lr: 0.02\n",
      "\tEpoch:  32 | \t Train Loss: 2.115 | \t  lr: 0.02\n",
      "\tEpoch:  33 | \t Train Loss: 2.082 | \t  lr: 0.02\n",
      "\tEpoch:  34 | \t Train Loss: 2.079 | \t  lr: 0.02\n",
      "\tEpoch:  35 | \t Train Loss: 2.061 | \t  lr: 0.02\n",
      "\tEpoch:  36 | \t Train Loss: 2.057 | \t  lr: 0.02\n",
      "\tEpoch:  37 | \t Train Loss: 2.042 | \t  lr: 0.02\n",
      "\tEpoch:  38 | \t Train Loss: 2.045 | \t  lr: 0.02\n",
      "\tEpoch:  39 | \t Train Loss: 2.036 | \t  lr: 0.02\n",
      "\tEpoch:  40 | \t Train Loss: 2.014 | \t  lr: 0.02\n",
      "\tEpoch:  41 | \t Train Loss: 2.050 | \t  lr: 0.02\n",
      "\tEpoch:  42 | \t Train Loss: 2.039 | \t  lr: 0.02\n",
      "\tEpoch:  43 | \t Train Loss: 1.999 | \t  lr: 0.02\n",
      "\tEpoch:  44 | \t Train Loss: 2.007 | \t  lr: 0.02\n",
      "\tEpoch:  45 | \t Train Loss: 1.994 | \t  lr: 0.02\n",
      "\tEpoch:  46 | \t Train Loss: 1.985 | \t  lr: 0.02\n",
      "\tEpoch:  47 | \t Train Loss: 1.994 | \t  lr: 0.02\n",
      "\tEpoch:  48 | \t Train Loss: 1.973 | \t  lr: 0.02\n",
      "\tEpoch:  49 | \t Train Loss: 1.961 | \t  lr: 0.02\n",
      "\tEpoch:  50 | \t Train Loss: 1.993 | \t  lr: 0.02\n",
      "\tEpoch:  51 | \t Train Loss: 1.982 | \t  lr: 0.02\n",
      "\tEpoch:  52 | \t Train Loss: 1.952 | \t  lr: 0.02\n",
      "\tEpoch:  53 | \t Train Loss: 1.956 | \t  lr: 0.02\n",
      "\tEpoch:  54 | \t Train Loss: 1.940 | \t  lr: 0.02\n",
      "\tEpoch:  55 | \t Train Loss: 1.966 | \t  lr: 0.02\n",
      "\tEpoch:  56 | \t Train Loss: 1.945 | \t  lr: 0.02\n",
      "\tEpoch:  57 | \t Train Loss: 1.910 | \t  lr: 0.02\n",
      "\tEpoch:  58 | \t Train Loss: 1.935 | \t  lr: 0.02\n",
      "\tEpoch:  59 | \t Train Loss: 1.937 | \t  lr: 0.02\n",
      "\tEpoch:  60 | \t Train Loss: 1.930 | \t  lr: 0.02\n",
      "\tEpoch:  61 | \t Train Loss: 1.907 | \t  lr: 0.02\n",
      "\tEpoch:  62 | \t Train Loss: 1.921 | \t  lr: 0.02\n",
      "\tEpoch:  63 | \t Train Loss: 1.917 | \t  lr: 0.02\n",
      "\tEpoch:  64 | \t Train Loss: 1.914 | \t  lr: 0.02\n",
      "\tEpoch:  65 | \t Train Loss: 1.886 | \t  lr: 0.02\n",
      "\tEpoch:  66 | \t Train Loss: 1.897 | \t  lr: 0.02\n",
      "\tEpoch:  67 | \t Train Loss: 1.919 | \t  lr: 0.02\n",
      "\tEpoch:  68 | \t Train Loss: 1.928 | \t  lr: 0.02\n",
      "\tEpoch:  69 | \t Train Loss: 1.907 | \t  lr: 0.02\n",
      "\tEpoch:  70 | \t Train Loss: 1.897 | \t  lr: 0.02\n",
      "\tEpoch:  71 | \t Train Loss: 1.884 | \t  lr: 0.02\n",
      "\tEpoch:  72 | \t Train Loss: 1.884 | \t  lr: 0.02\n",
      "\tEpoch:  73 | \t Train Loss: 1.863 | \t  lr: 0.02\n",
      "\tEpoch:  74 | \t Train Loss: 1.858 | \t  lr: 0.02\n",
      "\tEpoch:  75 | \t Train Loss: 1.884 | \t  lr: 0.02\n",
      "\tEpoch:  76 | \t Train Loss: 1.857 | \t  lr: 0.02\n",
      "\tEpoch:  77 | \t Train Loss: 1.886 | \t  lr: 0.02\n",
      "\tEpoch:  78 | \t Train Loss: 1.883 | \t  lr: 0.02\n",
      "\tEpoch:  79 | \t Train Loss: 1.892 | \t  lr: 0.02\n",
      "\tEpoch:  80 | \t Train Loss: 1.842 | \t  lr: 0.02\n",
      "\tEpoch:  81 | \t Train Loss: 1.868 | \t  lr: 0.02\n",
      "\tEpoch:  82 | \t Train Loss: 1.889 | \t  lr: 0.02\n",
      "\tEpoch:  83 | \t Train Loss: 1.887 | \t  lr: 0.02\n",
      "\tEpoch:  84 | \t Train Loss: 1.859 | \t  lr: 0.02\n",
      "\tEpoch:  85 | \t Train Loss: 1.856 | \t  lr: 0.02\n",
      "\tEpoch:  86 | \t Train Loss: 1.801 | \t  lr: 0.02\n",
      "\tEpoch:  87 | \t Train Loss: 1.853 | \t  lr: 0.02\n",
      "\tEpoch:  88 | \t Train Loss: 1.837 | \t  lr: 0.02\n",
      "\tEpoch:  89 | \t Train Loss: 1.876 | \t  lr: 0.02\n",
      "\tEpoch:  90 | \t Train Loss: 1.808 | \t  lr: 0.02\n",
      "\tEpoch:  91 | \t Train Loss: 1.878 | \t  lr: 0.02\n",
      "\tEpoch:  92 | \t Train Loss: 1.842 | \t  lr: 0.02\n",
      "\tEpoch:  93 | \t Train Loss: 1.825 | \t  lr: 0.02\n",
      "\tEpoch:  94 | \t Train Loss: 1.816 | \t  lr: 0.02\n",
      "\tEpoch:  95 | \t Train Loss: 1.804 | \t  lr: 0.02\n",
      "\tEpoch:  96 | \t Train Loss: 1.833 | \t  lr: 0.02\n",
      "\tEpoch:  97 | \t Train Loss: 1.808 | \t  lr: 0.002\n",
      "\tEpoch:  98 | \t Train Loss: 1.738 | \t  lr: 0.002\n",
      "\tEpoch:  99 | \t Train Loss: 1.686 | \t  lr: 0.002\n",
      "\tEpoch: 100 | \t Train Loss: 1.737 | \t  lr: 0.002\n",
      "\tEpoch: 101 | \t Train Loss: 1.699 | \t  lr: 0.002\n",
      "\tEpoch: 102 | \t Train Loss: 1.691 | \t  lr: 0.002\n",
      "\tEpoch: 103 | \t Train Loss: 1.719 | \t  lr: 0.002\n",
      "\tEpoch: 104 | \t Train Loss: 1.684 | \t  lr: 0.002\n",
      "\tEpoch: 105 | \t Train Loss: 1.700 | \t  lr: 0.002\n",
      "\tEpoch: 106 | \t Train Loss: 1.689 | \t  lr: 0.002\n",
      "\tEpoch: 107 | \t Train Loss: 1.674 | \t  lr: 0.002\n",
      "\tEpoch: 108 | \t Train Loss: 1.667 | \t  lr: 0.002\n",
      "\tEpoch: 109 | \t Train Loss: 1.680 | \t  lr: 0.002\n",
      "\tEpoch: 110 | \t Train Loss: 1.695 | \t  lr: 0.002\n",
      "\tEpoch: 111 | \t Train Loss: 1.679 | \t  lr: 0.002\n",
      "\tEpoch: 112 | \t Train Loss: 1.717 | \t  lr: 0.002\n",
      "\tEpoch: 113 | \t Train Loss: 1.711 | \t  lr: 0.002\n",
      "\tEpoch: 114 | \t Train Loss: 1.673 | \t  lr: 0.002\n",
      "\tEpoch: 115 | \t Train Loss: 1.656 | \t  lr: 0.002\n",
      "\tEpoch: 116 | \t Train Loss: 1.688 | \t  lr: 0.002\n",
      "\tEpoch: 117 | \t Train Loss: 1.695 | \t  lr: 0.002\n",
      "\tEpoch: 118 | \t Train Loss: 1.649 | \t  lr: 0.002\n",
      "\tEpoch: 119 | \t Train Loss: 1.635 | \t  lr: 0.002\n",
      "\tEpoch: 120 | \t Train Loss: 1.671 | \t  lr: 0.002\n",
      "\tEpoch: 121 | \t Train Loss: 1.665 | \t  lr: 0.002\n",
      "\tEpoch: 122 | \t Train Loss: 1.683 | \t  lr: 0.002\n",
      "\tEpoch: 123 | \t Train Loss: 1.656 | \t  lr: 0.002\n",
      "\tEpoch: 124 | \t Train Loss: 1.656 | \t  lr: 0.002\n",
      "\tEpoch: 125 | \t Train Loss: 1.644 | \t  lr: 0.002\n",
      "\tEpoch: 126 | \t Train Loss: 1.638 | \t  lr: 0.002\n",
      "\tEpoch: 127 | \t Train Loss: 1.665 | \t  lr: 0.002\n",
      "\tEpoch: 128 | \t Train Loss: 1.653 | \t  lr: 0.002\n",
      "\tEpoch: 129 | \t Train Loss: 1.648 | \t  lr: 0.002\n",
      "\tEpoch: 130 | \t Train Loss: 1.650 | \t  lr: 0.0002\n",
      "\tEpoch: 131 | \t Train Loss: 1.667 | \t  lr: 0.0002\n",
      "\tEpoch: 132 | \t Train Loss: 1.641 | \t  lr: 0.0002\n",
      "\tEpoch: 133 | \t Train Loss: 1.632 | \t  lr: 0.0002\n",
      "\tEpoch: 134 | \t Train Loss: 1.659 | \t  lr: 0.0002\n",
      "\tEpoch: 135 | \t Train Loss: 1.655 | \t  lr: 0.0002\n",
      "\tEpoch: 136 | \t Train Loss: 1.660 | \t  lr: 0.0002\n",
      "\tEpoch: 137 | \t Train Loss: 1.637 | \t  lr: 0.0002\n",
      "\tEpoch: 138 | \t Train Loss: 1.668 | \t  lr: 0.0002\n",
      "\tEpoch: 139 | \t Train Loss: 1.651 | \t  lr: 0.0002\n",
      "\tEpoch: 140 | \t Train Loss: 1.677 | \t  lr: 0.0002\n",
      "\tEpoch: 141 | \t Train Loss: 1.642 | \t  lr: 0.0002\n",
      "\tEpoch: 142 | \t Train Loss: 1.615 | \t  lr: 0.0002\n",
      "\tEpoch: 143 | \t Train Loss: 1.666 | \t  lr: 0.0002\n",
      "\tEpoch: 144 | \t Train Loss: 1.627 | \t  lr: 0.0002\n",
      "\tEpoch: 145 | \t Train Loss: 1.673 | \t  lr: 0.0002\n",
      "\tEpoch: 146 | \t Train Loss: 1.616 | \t  lr: 0.0002\n",
      "\tEpoch: 147 | \t Train Loss: 1.652 | \t  lr: 0.0002\n",
      "\tEpoch: 148 | \t Train Loss: 1.634 | \t  lr: 0.0002\n",
      "\tEpoch: 149 | \t Train Loss: 1.666 | \t  lr: 0.0002\n",
      "\tEpoch: 150 | \t Train Loss: 1.622 | \t  lr: 0.0002\n",
      "\tEpoch: 151 | \t Train Loss: 1.641 | \t  lr: 0.0002\n",
      "\tEpoch: 152 | \t Train Loss: 1.668 | \t  lr: 0.0002\n",
      "\tEpoch: 153 | \t Train Loss: 1.640 | \t  lr: 2e-05\n",
      "\tEpoch: 154 | \t Train Loss: 1.675 | \t  lr: 2e-05\n",
      "\tEpoch: 155 | \t Train Loss: 1.604 | \t  lr: 2e-05\n",
      "\tEpoch: 156 | \t Train Loss: 1.650 | \t  lr: 2e-05\n",
      "\tEpoch: 157 | \t Train Loss: 1.648 | \t  lr: 2e-05\n",
      "\tEpoch: 158 | \t Train Loss: 1.621 | \t  lr: 2e-05\n",
      "\tEpoch: 159 | \t Train Loss: 1.635 | \t  lr: 2e-05\n",
      "\tEpoch: 160 | \t Train Loss: 1.624 | \t  lr: 2e-05\n",
      "\tEpoch: 161 | \t Train Loss: 1.623 | \t  lr: 2e-05\n",
      "\tEpoch: 162 | \t Train Loss: 1.641 | \t  lr: 2e-05\n",
      "\tEpoch: 163 | \t Train Loss: 1.650 | \t  lr: 2e-05\n",
      "\tEpoch: 164 | \t Train Loss: 1.628 | \t  lr: 2e-05\n",
      "\tEpoch: 165 | \t Train Loss: 1.637 | \t  lr: 2e-05\n",
      "\tEpoch: 166 | \t Train Loss: 1.637 | \t  lr: 2.0000000000000003e-06\n",
      "\tEpoch: 167 | \t Train Loss: 1.644 | \t  lr: 2.0000000000000003e-06\n",
      "\tEpoch: 168 | \t Train Loss: 1.676 | \t  lr: 2.0000000000000003e-06\n",
      "\tEpoch: 169 | \t Train Loss: 1.633 | \t  lr: 2.0000000000000003e-06\n",
      "\tEpoch: 170 | \t Train Loss: 1.653 | \t  lr: 2.0000000000000003e-06\n",
      "\tEpoch: 171 | \t Train Loss: 1.625 | \t  lr: 2.0000000000000003e-06\n",
      "\tEpoch: 172 | \t Train Loss: 1.643 | \t  lr: 2.0000000000000003e-06\n",
      "\tEpoch: 173 | \t Train Loss: 1.635 | \t  lr: 2.0000000000000003e-06\n",
      "\tEpoch: 174 | \t Train Loss: 1.641 | \t  lr: 2.0000000000000003e-06\n",
      "\tEpoch: 175 | \t Train Loss: 1.658 | \t  lr: 2.0000000000000003e-06\n",
      "\tEpoch: 176 | \t Train Loss: 1.630 | \t  lr: 2.0000000000000003e-06\n",
      "\tEpoch: 177 | \t Train Loss: 1.607 | \t  lr: 2.0000000000000004e-07\n",
      "\tEpoch: 178 | \t Train Loss: 1.627 | \t  lr: 2.0000000000000004e-07\n",
      "\tEpoch: 179 | \t Train Loss: 1.624 | \t  lr: 2.0000000000000004e-07\n",
      "\tEpoch: 180 | \t Train Loss: 1.648 | \t  lr: 2.0000000000000004e-07\n",
      "\tEpoch: 181 | \t Train Loss: 1.598 | \t  lr: 2.0000000000000004e-07\n",
      "\tEpoch: 182 | \t Train Loss: 1.624 | \t  lr: 2.0000000000000004e-07\n",
      "\tEpoch: 183 | \t Train Loss: 1.635 | \t  lr: 2.0000000000000004e-07\n",
      "\tEpoch: 184 | \t Train Loss: 1.620 | \t  lr: 2.0000000000000004e-07\n",
      "\tEpoch: 185 | \t Train Loss: 1.617 | \t  lr: 2.0000000000000004e-07\n",
      "\tEpoch: 186 | \t Train Loss: 1.600 | \t  lr: 2.0000000000000004e-07\n",
      "\tEpoch: 187 | \t Train Loss: 1.622 | \t  lr: 2.0000000000000004e-07\n",
      "\tEpoch: 188 | \t Train Loss: 1.614 | \t  lr: 2.0000000000000004e-07\n",
      "\tEpoch: 189 | \t Train Loss: 1.634 | \t  lr: 2.0000000000000004e-07\n",
      "\tEpoch: 190 | \t Train Loss: 1.703 | \t  lr: 2.0000000000000004e-07\n",
      "\tEpoch: 191 | \t Train Loss: 1.628 | \t  lr: 2.0000000000000004e-07\n",
      "\tEpoch: 192 | \t Train Loss: 1.650 | \t  lr: 2.0000000000000007e-08\n",
      "\tEpoch: 193 | \t Train Loss: 1.634 | \t  lr: 2.0000000000000007e-08\n",
      "\tEpoch: 194 | \t Train Loss: 1.618 | \t  lr: 2.0000000000000007e-08\n",
      "\tEpoch: 195 | \t Train Loss: 1.627 | \t  lr: 2.0000000000000007e-08\n",
      "\tEpoch: 196 | \t Train Loss: 1.655 | \t  lr: 2.0000000000000007e-08\n",
      "\tEpoch: 197 | \t Train Loss: 1.640 | \t  lr: 2.0000000000000007e-08\n",
      "\tEpoch: 198 | \t Train Loss: 1.656 | \t  lr: 2.0000000000000007e-08\n",
      "\tEpoch: 199 | \t Train Loss: 1.625 | \t  lr: 2.0000000000000007e-08\n"
     ]
    }
   ],
   "source": [
    "from helpers import evaluate, train_loop\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "model_out_path = 'qna-model.pt'\n",
    "# definer optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# ignore padding index when calculating the loss (<PAD>=3 in vocab)\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=3)\n",
    "# test model with logsoftmax and see if it does help the training to converge better\n",
    "criterion = nn.NLLLoss(ignore_index=1)\n",
    "# lr scheduler to see if this improves the performance of the validation loss\n",
    "lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=10)\n",
    "# train loop\n",
    "train_loop(model, train_dataloader, test_dataloader, optimizer, criterion, lr_scheduler, clip, teaching_ratio, device, epochs, model_out_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 4.011\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on data it has never seen\n",
    "model_out_path = 'qna-model.pt'\n",
    "model.load_state_dict(torch.load(model_out_path))\n",
    "valid_loss = evaluate(model, test_dataloader, criterion, device)\n",
    "print(f'Validation Loss: {valid_loss:.3f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 'exit' to finish the chat.\n",
      " ------------------------------ \n",
      "\n",
      "Question:\t How did Beyonc√© name her daughter?\n",
      "Answer:\t  wolf singl conservatori        \n",
      "Question:\t Who led Issacs troops to Cyprus?\n",
      "Answer:\t  us 3 million price       \n",
      "Question:\t Who began a program of church reform in the 1100s?\n",
      "Answer:\t  119 21 1991 coup       \n",
      "Question:\t Who made fun of the Latin language?\n",
      "Answer:\t  10 30 1991 000       \n",
      "Question:\t Which first chopin work did he gain international renown for?\n",
      "Answer:\t  new york client repres       \n",
      "Question:\t Which instrument for disco songs do incorporate house music?\n",
      "Answer:\t  almost arkadievich malyarchuk hors       \n",
      "Question:\t How many people watched the first episode of american idol?\n",
      "Answer:\t  us 000 million 000       \n"
     ]
    }
   ],
   "source": [
    "# inference, load model\n",
    "model = Seq2Seq(enc, dec, device)\n",
    "model.load_state_dict(torch.load(model_out_path))\n",
    "model.eval\n",
    "\n",
    "# define the input question\n",
    "# question = \"What does the urban education institute help run?\"\n",
    "print(\"Type 'exit' to finish the chat.\\n\", \"-\"*30, '\\n')\n",
    "while (True):\n",
    "    question = input(\"> \")\n",
    "    if question.strip() == \"exit\":\n",
    "        break\n",
    "    # clean and tokenize the input question\n",
    "    print(f'Question:\\t {question}')\n",
    "    src = v.word2index(v.clean_text(question))\n",
    "    # print(f'Clean Question: {src}')\n",
    "    # convert the tokenized question to a tensor and add a batch dimension\n",
    "    src = torch.tensor(src, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    # generate the answer using the model\n",
    "    output = model(src=src, trg=None, teaching=0, max_len=12)\n",
    "    # convert the output tensor to a list of token IDs\n",
    "    preds = output.argmax(dim=1).tolist()[0]\n",
    "    # convert the token IDs to tokens\n",
    "    answer = v.index2word(preds)\n",
    "    # pretty answer\n",
    "    pretty_answer = ' '.join([w for w in answer]).replace('<SOS>', '').replace('<EOS>', '').replace('<PAD>','')\n",
    "    # print the predicted answer\n",
    "    print(f'Answer:\\t {pretty_answer}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sample questions for chatbot\n",
    "Who led Issacs troops to Cyprus?\n",
    "How did Beyonc√© name her daughter?\n",
    "Who began a program of church reform in the 1100s?\n",
    "Who made fun of the Latin language?\n",
    "Which first chopin work did he gain international renown for?\n",
    "Which instrument for disco songs do incorporate house music?\n",
    "How many people watched the first episode of american idol?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(Starter Code) LSTM Bot",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}